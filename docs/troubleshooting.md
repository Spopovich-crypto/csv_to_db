# トラブルシューティング

このドキュメントでは、CSV to DBツールの使用中に発生する可能性のある一般的な問題と、その解決方法について説明します。

## 目次

- [一般的な問題と解決方法](#一般的な問題と解決方法)
  - [CSVファイルが見つからない](#1-csvファイルが見つからない)
  - [データベース接続エラー](#2-データベース接続エラー)
  - [CSVファイルの読み込みエラー](#3-csvファイルの読み込みエラー)
  - [ZIPファイル内のCSVファイル処理エラー](#4-zipファイル内のcsvファイル処理エラー)
  - [日時の変換エラー](#5-日時の変換エラー)
  - [データベースへのインポートエラー](#6-データベースへのインポートエラー)
  - [処理済みファイルが再処理されない](#7-処理済みファイルが再処理されない)
- [エラーメッセージの一覧と意味](#エラーメッセージの一覧と意味)
- [ログの見方](#ログの見方)
- [よくある質問（FAQ）](#よくある質問faq)
- [サポート情報](#サポート情報)
- [次のステップ](#次のステップ)

## 一般的な問題と解決方法

### 1. CSVファイルが見つからない

**症状**: 以下のようなメッセージが表示される
```
条件に一致するCSVファイルは見つかりませんでした。
```

**考えられる原因**:
- 指定したフォルダ内にCSVファイルが存在しない
- CSVファイル名が`PATTERN`設定に一致しない
- `FOLDER`パスが間違っている

**解決方法**:
1. `.env`ファイルの`FOLDER`設定が正しいことを確認する
2. `.env`ファイルの`PATTERN`設定が正しいことを確認する
3. 指定したフォルダ内にCSVファイルが存在することを確認する
4. CSVファイル名が`PATTERN`設定に一致することを確認する

### 2. データベース接続エラー

**症状**: 以下のようなメッセージが表示される
```
データベース接続エラー: [エラーメッセージ]
```

**考えられる原因**:
- データベースファイルのパスが間違っている
- データベースファイルが破損している
- データベースディレクトリが存在しない

**解決方法**:
1. `.env`ファイルの`DB`設定が正しいことを確認する
2. データベースディレクトリが存在することを確認する
3. 破損している可能性がある場合は、データベースファイルを削除して再作成する

### 3. CSVファイルの読み込みエラー

**症状**: 以下のようなメッセージが表示される
```
特殊CSVファイル読み込みエラー [ファイルパス]: [エラーメッセージ]
```

**考えられる原因**:
- CSVファイルのフォーマットが正しくない
- CSVファイルのエンコーディングが`.env`ファイルの`ENCODING`設定と一致しない
- CSVファイルが破損している

**解決方法**:
1. CSVファイルのフォーマットが[ファイル形式ガイド](file_formats.md)に記載されている形式に一致することを確認する
2. `.env`ファイルの`ENCODING`設定がCSVファイルのエンコーディングと一致することを確認する
3. CSVファイルが破損していないことを確認する

### 4. ZIPファイル内のCSVファイル処理エラー

**症状**: 以下のようなメッセージが表示される
```
ZIPファイル内のCSV処理エラー [ファイルパス]: [エラーメッセージ]
```

**考えられる原因**:
- ZIPファイルが破損している
- ZIPファイル内のCSVファイルのフォーマットが正しくない
- ZIPファイル内のCSVファイルのエンコーディングが`.env`ファイルの`ENCODING`設定と一致しない

**解決方法**:
1. ZIPファイルが破損していないことを確認する
2. ZIPファイル内のCSVファイルのフォーマットが[ファイル形式ガイド](file_formats.md)に記載されている形式に一致することを確認する
3. `.env`ファイルの`ENCODING`設定がZIPファイル内のCSVファイルのエンコーディングと一致することを確認する

### 5. 日時の変換エラー

**症状**: 以下のようなメッセージが表示される
```
日時の変換に失敗しました: [エラーメッセージ]。文字列のまま処理を続行します。
```

**考えられる原因**:
- CSVファイル内の日時形式が対応していない形式である

**解決方法**:
1. CSVファイル内の日時形式が以下のいずれかの形式であることを確認する
   - `YYYY/MM/DD HH:MM:SS`（例: `2024/04/01 10:00:00`）
   - `YYYY-MM-DD HH:MM:SS`（例: `2024-04-01 10:00:00`）
2. 日時形式が異なる場合は、CSVファイルを編集して対応する形式に変更する

### 6. データベースへのインポートエラー

**症状**: 以下のようなメッセージが表示される
```
データフレームインポートエラー: [エラーメッセージ]
```

**考えられる原因**:
- データベースファイルが破損している
- データベースファイルへの書き込み権限がない
- データベーステーブルのスキーマが変更されている

**解決方法**:
1. データベースファイルが破損していないことを確認する
2. データベースファイルへの書き込み権限があることを確認する
3. データベースファイルを削除して再作成する

### 7. 処理済みファイルが再処理されない

**症状**: 既に処理したファイルが再度処理されない

**考えられる原因**:
- ファイルが処理済みとしてマークされている

**解決方法**:
1. `processed_files.json`ファイルを削除する
2. または、`processed_files.json`ファイルから該当するファイルのエントリを削除する
3. または、CSVファイルを変更する（ハッシュ値が変わるため、新しいファイルとして認識される）

## エラーメッセージの一覧と意味

以下に、CSV to DBツールが出力する主なエラーメッセージとその意味を示します：

| エラーメッセージ | 意味 | 対処方法 |
|----------------|------|---------|
| `.envファイルにFOLDERまたはPATTERNが設定されていません。` | 必須の環境変数が設定されていない | `.env`ファイルに`FOLDER`と`PATTERN`を設定する |
| `指定されたフォルダが存在しません: [フォルダパス]` | 指定されたフォルダが存在しない | 正しいフォルダパスを指定する |
| `データベース接続エラー: [エラーメッセージ]` | データベースへの接続に失敗した | データベースファイルのパスを確認する |
| `特殊CSVファイル読み込みエラー [ファイルパス]: [エラーメッセージ]` | CSVファイルの読み込みに失敗した | CSVファイルのフォーマットとエンコーディングを確認する |
| `ZIPファイル内のCSV処理エラー [ファイルパス]: [エラーメッセージ]` | ZIPファイル内のCSVファイルの処理に失敗した | ZIPファイルとその中のCSVファイルを確認する |
| `日時の変換に失敗しました: [エラーメッセージ]` | 日時の変換に失敗した | CSVファイル内の日時形式を確認する |
| `データフレームインポートエラー: [エラーメッセージ]` | データベースへのインポートに失敗した | データベースファイルを確認する |
| `Out of Memory Error: failed to offload data block of size X KiB (Y GiB / Y GiB used)` | テンポラリディレクトリのサイズ制限に達した | テンポラリディレクトリのサイズ制限を調整するか、クエリを最適化する |
| `ファイル処理中にエラーが発生しました [ファイルパス]: [エラーメッセージ]` | ファイル処理中に一般的なエラーが発生した | エラーメッセージを確認して対処する |

## ログの見方

CSV to DBツールは、実行中の情報をログとして出力します。ログは以下の形式で出力されます：

```
YYYY-MM-DD HH:MM:SS - レベル - メッセージ
```

ログレベルには以下の種類があります：

- **DEBUG**: デバッグ情報（詳細な情報）
- **INFO**: 情報メッセージ（通常の実行情報）
- **WARNING**: 警告メッセージ（問題が発生する可能性がある）
- **ERROR**: エラーメッセージ（処理が失敗した）
- **CRITICAL**: 致命的なエラーメッセージ（プログラムが終了する可能性がある）

ログメッセージの例：

```
2025-04-03 10:00:00 - INFO - 設定情報:
2025-04-03 10:00:00 - INFO - FOLDER: data
2025-04-03 10:00:00 - INFO - PATTERN: (Cond|User|test)
2025-04-03 10:00:00 - INFO - 以下のフォルダ内でパターン'(Cond|User|test)'に一致するCSVファイルを検索中...
2025-04-03 10:00:01 - INFO - 見つかったCSVファイル (2件):
2025-04-03 10:00:01 - INFO - 1. test_data.csv
2025-04-03 10:00:01 - INFO - 2. test_data.zip::Cond_data.csv (ZIPファイル内)
2025-04-03 10:00:02 - INFO - データベースに接続しました: database/sensor_data.duckdb
2025-04-03 10:00:02 - INFO - CSVファイルの前処理と直接DBへの投入を開始します...
2025-04-03 10:00:03 - INFO - ファイルの前処理が完了しました: data/test_data.csv -> processed/processed_test_data.csv
2025-04-03 10:00:04 - INFO - テーブル sensor_data_integrated の構造:
2025-04-03 10:00:04 - INFO - インポートされたレコード数: 1000
2025-04-03 10:00:04 - INFO - データベース接続を閉じました
2025-04-03 10:00:04 - INFO - データベースへの直接取り込みが完了しました。
```

## よくある質問（FAQ）

### Q: 処理済みのCSVファイルを再度処理するにはどうすればよいですか？

A: 以下のいずれかの方法を使用します：

1. `processed_files.json`ファイルを削除する
2. `processed_files.json`ファイルから該当するファイルのエントリを削除する
3. CSVファイルを変更する（ハッシュ値が変わるため、新しいファイルとして認識される）

### Q: 特定のパターンに一致するCSVファイルのみを処理するにはどうすればよいですか？

A: `.env`ファイルの`PATTERN`設定を変更します。例えば、`User`という文字列を含むCSVファイルのみを処理する場合は、以下のように設定します：

```
PATTERN=User
```

### Q: 処理結果のParquetファイルをどのように確認できますか？

A: Parquetファイルは、PandasやPolarsなどのデータ分析ライブラリで読み込むことができます。以下は、Polarsを使用してParquetファイルを読み込む例です：

```python
import polars as pl

# Parquetファイルの読み込み
df = pl.read_parquet("processed/integrated_data.parquet")
print(df)
```

### Q: データベースのデータをどのように確認できますか？

A: DuckDBは標準的なSQLクエリをサポートしているため、以下のようにしてデータにアクセスできます：

```python
import duckdb

# データベースに接続
conn = duckdb.connect("database/sensor_data.duckdb")

# データの取得
result = conn.execute("SELECT * FROM sensor_data_integrated LIMIT 10").fetchall()
print(result)

# 接続を閉じる
conn.close()
```

### Q: 日本語のCSVファイルを処理する際に文字化けが発生します。どうすればよいですか？

A: `.env`ファイルの`ENCODING`設定を、CSVファイルのエンコーディングに合わせて変更します。例えば、Shift-JISエンコーディングのCSVファイルを処理する場合は、以下のように設定します：

```
ENCODING=shift-jis
```

### Q: 大量のCSVファイルを処理する際にメモリ不足エラー（Out of Memory Error）が発生します。どうすればよいですか？

A: バージョン2.0.0以降では、メモリ最適化機能が実装されており、以下の設定で調整できます：

1. `.env`ファイルでメモリ最適化設定を調整する：
   ```
   # メモリ最適化設定
   BATCH_SIZE=5     # 一度に処理するCSVファイル数（小さくするとメモリ使用量が減少）
   CHUNK_SIZE=10000 # 一度に処理するデータ行数（小さくするとメモリ使用量が減少）
   ```

2. 実データのサイズに応じて、これらの値を調整してください：
   - 非常に大きなCSVファイル（数百MB以上）の場合：`BATCH_SIZE=1`、`CHUNK_SIZE=5000`
   - 中程度のCSVファイル（数十MB）の場合：`BATCH_SIZE=3`、`CHUNK_SIZE=10000`
   - 小さなCSVファイル（数MB以下）の場合：`BATCH_SIZE=10`、`CHUNK_SIZE=20000`

3. それでもメモリ不足エラーが発生する場合は、以下の方法も試してください：
   - 不要なプロセスを終了してメモリを解放する
   - メモリ容量の大きいマシンで実行する
   - CSVファイルを手動で小さなファイルに分割する

### Q: メモリ最適化設定を使用すると処理速度が遅くなりますか？

A: はい、メモリ使用量と処理速度はトレードオフの関係にあります：

- `BATCH_SIZE`と`CHUNK_SIZE`を小さくすると、メモリ使用量は減少しますが、処理速度は遅くなる傾向があります
- 逆に、これらの値を大きくすると、処理速度は向上しますが、メモリ使用量も増加します

最適な設定は、使用するマシンのメモリ容量とCSVファイルのサイズによって異なります。まずは推奨値から始めて、必要に応じて調整してください。

### Q: 「Out of Memory Error: Allocation failure」というエラーが発生しました。これは何ですか？

A: このエラーは、プログラムがメモリを割り当てようとしたときに、利用可能なメモリが不足していることを示しています。CSV to DBツールでは、以下の原因が考えられます：

1. 非常に大きなCSVファイルを処理している
2. 多数のCSVファイルを一度に処理している
3. 縦持ちデータへの変換時に大量のメモリを使用している
4. DuckDBがデータベース操作で大量のメモリを使用している

このエラーを解決するには、上記のメモリ最適化設定を調整してください。特に`BATCH_SIZE`を小さくすることで、一度に処理するファイル数を減らし、メモリ使用量を削減できます。

### Q: 「Out of Memory Error: failed to offload data block」というエラーが発生しました。これは何ですか？

A: このエラーは、DuckDBがテンポラリディレクトリに中間データを保存する際に、設定されたサイズ制限（`max_temp_directory_size`）に達したことを示しています。特に重複削除などの大規模なデータ処理を行う際に発生することがあります。

このエラーを解決するには、以下の方法があります：

1. **クエリの最適化**: データを日付ごとに分割して処理することで、一度に使用するディスク容量を削減できます。バージョン2.1.0以降では、重複削除処理が自動的に日付ごとの分割処理を行うように最適化されています。

2. **テンポラリディレクトリのサイズ制限を調整**: `.env`ファイルに以下の設定を追加することで、テンポラリディレクトリのサイズ制限を調整できます：
   ```
   # DuckDB設定
   MAX_TEMP_DIRECTORY_SIZE=20GB
   ```
   ただし、物理的なディスク容量を超える設定はできません。

3. **バッチサイズとチャンクサイズの削減**: `.env`ファイルの`BATCH_SIZE`と`CHUNK_SIZE`をさらに小さくすることで、一度に処理するデータ量を減らせます。

## サポート情報

さらに支援が必要な場合は、以下の方法でサポートを受けることができます：

1. GitHubのIssueを作成する
2. プロジェクトのメンテナーに連絡する
3. コミュニティフォーラムで質問する

## 次のステップ

- [ユーザーマニュアル](user_manual.md)に戻って、ツールの基本的な使い方を確認する
- [開発者ガイド](developer_guide.md)を参照して、ツールの拡張方法について学ぶ
